# -*- coding: utf-8 -*-
"""NLP

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JUqJDcA57QXYYHFHpkXYwjCTnf_S0ccJ
"""

!nvidia-smi -L

!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py

from helper_functions import

!wget "https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip"

import zipfile

zip_nlp = zipfile.ZipFile("nlp_getting_started.zip","r")
zip_nlp.extractall()
zip_nlp.close()

import pandas as pd
train_nlp = pd.read_csv("train.csv")
test_nlp = pd.read_csv("test.csv")
train_nlp.head()

train_nlp_shuffled = train_nlp.sample(frac=1, random_state=42)
train_nlp_shuffled.head()

test_nlp.head()

train_nlp.target.value_counts()

len(train_nlp),len(test_nlp),len(train_nlp)+len(test_nlp)

from sklearn.model_selection import train_test_split

x = train_nlp_shuffled['text']
y = train_nlp_shuffled["target"]

x_train, x_val, y_train, y_val = train_test_split(x,
                                                  y,
                                                  test_size=0.1)

import tensorflow as tf

from tensorflow.keras import layers

text_vector = layers.TextVectorization(max_tokens=None,
                                standardize="lower_and_strip_punctuation",
                                split="whitespace",
                                ngrams=None,
                                output_mode="int",
                                output_sequence_length=None
                                )

for i in  x_train:
  total_words=sum([len(i.split())])
  

max_vocab_per_sent = round(total_words/len(x_train))
print(max_vocab_per_sent)

max_vocab_length = 10000
max_length = 15

text_vector_1 = TextVectorization(max_tokens=max_vocab_length,
                                  output_mode="int",
                                  output_sequence_length=max_length
                                  )

text_vector_1.adapt(x_train)

tf.random.set_seed(42)

embedding = layers.Embedding(input_dim=max_vocab_length,
                           output_dim=128,
                           embeddings_initializer="uniform",
                           input_length=max_length,
                           name="embedding_1"  
                           )

"""model_0:Naive Bayes(Baseline)"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline

model_0 = Pipeline([
    ("tfidf", TfidfVectorizer()),
    ("clf", MultinomialNB())
])

model_0.fit(x_train,y_train)

model_0_score = model_0.score(x_val,y_val)
model_0_score

model_0_preds = model_0.predict(x_val)
model_0_preds[:10]

from sklearn.metrics import accuracy_score,precision_recall_fscore_support

def evaluate(y_true, y_pred):
  model_accuracy = accuracy_score(y_true, y_pred)*100
  model_precision, model_recall, model_f1,_ = precision_recall_fscore_support(y_true, y_pred, average="weighted")
  model_evaluate = {"accuracy": model_accuracy,
                   "precision" : model_precision,
                   "recall"  : model_recall,
                   "f1" : model_f1}
  return model_evaluate

model_0_evaluate = evaluate(y_true=y_val, y_pred=model_0_preds)
model_0_evaluate

inputs = layers.Input(shape=(1,),dtype="string")
x = text_vector_1(inputs)
x = embedding(x)
x = layers.GlobalAveragePooling1D()(x)
outputs = layers.Dense(1,activation="sigmoid")(x)
model_1 = tf.keras.Model(inputs,outputs)

model_1.compile(loss="binary_crossentropy",
                optimizer=tf.keras.optimizers.Adam(),
                metrics=['accuracy'])

model_1.summary()

model_1_trained = model_1.fit(x_train,
                          y_train,
                          epochs=5,
                          validation_data=(x_val,y_val))



model_1.evaluate(x_val,y_val)

model_1_preds = model_1.predict(x_val)
model_1_preds[:10]

model_1_preds = tf.squeeze(tf.round(model_1_preds))
model_1_preds[:10]

model_1_evaluate = evaluate(y_true=y_val, y_pred=model_1_preds)
model_1_evaluate

"""Model_2:RNN_LSTM


"""

tf.random.set_seed(42)
embedding_2 = layers.Embedding(input_dim=max_vocab_length,
                               output_dim=128,
                               embeddings_initializer="uniform",
                               input_length=max_length,
                               name="embedding_2"  
                           )

model_1_trained.save("model_1_SavedModel_format")